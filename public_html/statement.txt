

1) Choose one of the following options:

a) Generation of networks using the Barabasi & Albert preferential attachment model. 
Compute the degree distribution and the complementary cumulative degree distribution, using histograms in log scale. 
Fitting of the power-law exponent from the histograms, and using the maximum likelihood estimation (MLE) as it appears in Newman's review. 
Build networks of different sizes (from 1000 to 100000 nodes), and with different average degree (2, 4, 6, ...).



# TODO

	Distribution: Barabasi & Albert 
	Compute:
		Degree distribution
		Complementary cumultive degree


	Algorithm:
		http://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model
		join(depending probability, skip or join if hit )
			


	Generating probabilities & depending to the weigth





# Deadline:
	- 1 month
	- any language
# Constrictions
	- individual
	- not use network-x where the implementation is 1 line
# Hint
	- generate random numbers with random weigths	
		- ID_node (i=0, ..., i=n)	
		- Ki_node (degree)
		- Si_Ki	  (Acumulative degree sum)


	ID	Ki	Si
	1	2	2	
	2	2	4
	3	3	7
	4	5	12	
	5	6	18
	6	1	19	
	7	1	20
	8	2	22	

	...



# Proposal
	- use javascript & Sigma.js
	- generate it in a JSON object {format???, ADJACENCY-LIST (List ++ LinkedList) } | In memory
		- srcNode(NODE)
		- tgtNode(NODE)

	- convertToPajek.js
		- input 	object.JSON
		- output	pajek.net
				* Vertices	100
				(type #length)
				1 1
				1 2
				...
				100 100
				(idx - id)
				* Edges
				1 3 1
				1 100 1
				(src - tgt - weigth)






b) Implementation of the Configuration Model. Generate networks of different sizes following Poisson degree distributions (Erdos-Renyi) and power-law degree distributions with different exponents (2.2, 2.5, 2.7). For the latter, calculate the exponents as in exercise 1a.

	- 1st decide the degree distribution
	- accumulative aristas connections " the number of stubs must be even"
	- use list + auxiliar list
	- avoid multiple links & self loops




# 
# configuration model


# the most important part are the statistics and generating the dataset for the posterior, pajek handling
